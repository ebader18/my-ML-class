{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "* Use Stanford dog dataset with over 20k images of dogs, but only use 4096 images.\n",
    "* Resize images to shape 12288 (64*64*3).\n",
    "* Copy all resized images to numpy array of shape (12288, 4096).\n",
    "* Any prediction above 50% is good!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version of stanford dog dataset\n",
    "path_dogs_dataset = kagglehub.dataset_download(\"jessicali9530/stanford-dogs-dataset\")\n",
    "\n",
    "# Download latest version of stanford car dataset\n",
    "path_cars_dataset = kagglehub.dataset_download(\"jessicali9530/stanford-cars-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store images and targets to hdf5 format\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "np.random.seed(18)\n",
    "SHAPE_IMAGES = (64, 64)\n",
    "\n",
    "def generate_dataset(directory, shape=(64, 64), num_img=4096):\n",
    "    image_paths = glob.glob(os.path.join(directory, '**', '*.*'), recursive=True)\n",
    "    image_paths = [p for p in image_paths if p.lower().endswith(('.jpeg', '.jpg'))]\n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        if len(images) >= num_img:\n",
    "            break\n",
    "        img = Image.open(path).convert('RGB').resize(shape)  # Convert to RGB and resize\n",
    "        images.append(np.array(img).flatten())  # Flatten into a 12288 (64x64x3) array\n",
    "\n",
    "    return np.array(images).T  # Shape 12288 x NUM_IMAGES\n",
    "\n",
    "Xdogs = generate_dataset(path_dogs_dataset, shape=SHAPE_IMAGES)\n",
    "Xcars = generate_dataset(path_cars_dataset, shape=SHAPE_IMAGES)\n",
    "X = np.hstack((Xdogs, Xcars)) / 255. # Normalize data from 0.0 to 1.0\n",
    "Ydogs = np.ones((1, Xdogs.shape[1]), dtype='int8')\n",
    "Ycars = np.zeros((1, Xcars.shape[1]), dtype='int8')\n",
    "Y = np.hstack((Ydogs, Ycars))\n",
    "\n",
    "shuffled_indices = np.random.permutation(X.shape[1])\n",
    "X = X[:, shuffled_indices]\n",
    "Y = Y[:, shuffled_indices]\n",
    "\n",
    "split_index = int(0.8 * X.shape[1])\n",
    "Xtrain, Xtest = X[:, :split_index], X[:, split_index:]\n",
    "Ytrain, Ytest = Y[:, :split_index], Y[:, split_index:]\n",
    "\n",
    "# Open the HDF5 file for writing (creates the file if it doesn't exist)\n",
    "with h5py.File('dogs.h5', 'w') as hf:\n",
    "    hf.create_dataset('Xtrain', data=Xtrain)\n",
    "    hf.create_dataset('Xtest', data=Xtest)\n",
    "    hf.create_dataset('Ytrain', data=Ytrain)\n",
    "    hf.create_dataset('Ytest', data=Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "* A single node made of a matrix W of shape (12288, 1) and a bias b\n",
    "* Initialize W and b to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import h5py\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def load_dataset(shape_img=(64, 64), show_samples=False):\n",
    "    with h5py.File('dogs.h5', 'r') as hf:\n",
    "        Xtrain = hf['Xtrain'][:].astype(np.float32)\n",
    "        Xtest = hf['Xtest'][:].astype(np.float32)\n",
    "        Ytrain = hf['Ytrain'][:].astype(np.int8)\n",
    "        Ytest = hf['Ytest'][:].astype(np.int8)\n",
    "    \n",
    "    if show_samples:\n",
    "        # Plot first 5 images of each dataset for verification\n",
    "        fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
    "        for i in range(10):\n",
    "            x = Xtrain[:, i].reshape(shape_img[0], shape_img[1], 3)\n",
    "            axes[0, i].imshow(x)\n",
    "            axes[0, i].axis('off')  # Hide the axis\n",
    "            axes[0, i].set_title(\"Dog\" if Ytrain[0, i] == 1 else \"Car\", fontsize=8)\n",
    "            x = Xtest[:, i].reshape(shape_img[0], shape_img[1], 3)\n",
    "            axes[1, i].imshow(x)\n",
    "            axes[1, i].axis('off')  # Hide the axis\n",
    "            axes[1, i].set_title(\"Dog\" if Ytest[0, i] == 1 else \"Car\", fontsize=8)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    return Xtrain, Xtest, Ytrain, Ytest\n",
    "\n",
    "def linear_combination(X, W, b):\n",
    "    Z = np.dot(W.T, X) + b\n",
    "\n",
    "    return Z\n",
    "\n",
    "def sigmoid(Z): # Compute the sigmoid of z\n",
    "    A = 1.0 / (1.0 + np.exp(-Z))\n",
    "    \n",
    "    return A\n",
    "\n",
    "def compute_cost(Y, A):\n",
    "    J = -np.sum(Y * np.log(A) + (1.0 - Y) * np.log(1.0 - A)) / A.shape[1]\n",
    "    \n",
    "    return J\n",
    "\n",
    "def compute_accuracy(Xtrain, Xtest, Ytrain, Ytest, W, b): # Compute accuracy: (True Positives + True Negatives) / Total predictions\n",
    "    Ypred_train = predict(Xtrain, W, b)\n",
    "    Ypred_test = predict(Xtest, W, b)\n",
    "    \n",
    "    acc_train = 100 - np.mean(np.abs(Ypred_train - Ytrain)) * 100\n",
    "    acc_test = 100 - np.mean(np.abs(Ypred_test - Ytest)) * 100\n",
    "\n",
    "    return acc_train, acc_test\n",
    "\n",
    "def compute_recall(Xtrain, Xtest, Ytrain, Ytest, W, b): # Compute accuracy: True Positives / (True Positives + False Negatives)\n",
    "    Ypred_train = predict(Xtrain, W, b)\n",
    "    Ypred_test = predict(Xtest, W, b)\n",
    "    \n",
    "    TP = np.sum((Ytrain == 1) & (Ypred_train == 1)) # True Positives: Actual dog (1) correctly predicted as dog (1)\n",
    "    FN = np.sum((Ytrain == 1) & (Ypred_train == 0)) # False Negatives: Actual dog (1) predicted as not dog (0)\n",
    "    recall_train = TP / (TP + FN) if (TP + FN) > 0 else 0   # Recall calculation\n",
    "\n",
    "    TP = np.sum((Ytest == 1) & (Ypred_test == 1)) # True Positives: Actual dog (1) correctly predicted as dog (1)\n",
    "    FN = np.sum((Ytest == 1) & (Ypred_test == 0)) # False Negatives: Actual dog (1) predicted as not dog (0)\n",
    "    recall_test = TP / (TP + FN) if (TP + FN) > 0 else 0   # Recall calculation\n",
    "\n",
    "    return 100. * recall_train, 100. * recall_test\n",
    "\n",
    "def predict(X, W, b, labels=None, shape_img=(64, 64), show_samples=False):\n",
    "    _, m = X.shape\n",
    "\n",
    "    # Forward propagation\n",
    "    Z = linear_combination(X, W, b) # Z = (1, m)\n",
    "    A = sigmoid(Z) # A = (1, m)\n",
    "\n",
    "    Ypred = np.zeros((1, m))\n",
    "    Ypred = np.where(A > 0.5, 1, 0)\n",
    "\n",
    "    if show_samples:\n",
    "        # Plot first 10 predictions\n",
    "        fig, axes = plt.subplots(1, 10, figsize=(20, 4))\n",
    "        for i in range(10):\n",
    "            x = X[:, i].reshape(shape_img[0], shape_img[1], 3)\n",
    "            axes[i].imshow(x)\n",
    "            axes[i].axis('off')  # Hide the axis\n",
    "            title = f'GT: {labels[Ytest[0, i]]}\\nPred: {labels[Ypred[0, i]]}'\n",
    "            axes[i].set_title(title)\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    return Ypred\n",
    "\n",
    "def plot_cost_function(performance_metrics, lr = 0.01):\n",
    "    costs = [metrics[0] for metrics in performance_metrics]\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(f'Cost Function over Iteration\\nLearning rate = {lr}')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_accuracy(performance_metrics, lr = 0.01):\n",
    "    accs_train = [metrics[1] for metrics in performance_metrics]\n",
    "    accs_test = [metrics[2] for metrics in performance_metrics]\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot accs_train on the left y-axis\n",
    "    ax1.plot(accs_train, 'b-', label='Train Accuracy')\n",
    "    ax1.set_xlabel('iterations (per hundreds)')\n",
    "    ax1.set_ylabel('Train Accuracy [%]', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "    # Set x-axis to logarithmic scale\n",
    "    ax1.set_xscale('log')\n",
    "\n",
    "    # Create a second y-axis for accs_test\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(accs_test, 'r-', label='Test Accuracy')\n",
    "    ax2.set_ylabel('Test Accuracy [%]', color='r')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Add a title and show the plot\n",
    "    plt.title(f'Accuracy\\nLearning rate = {lr}')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_recall(performance_metrics, lr = 0.01):\n",
    "    recall_train = [metrics[3] for metrics in performance_metrics]\n",
    "    recall_test = [metrics[4] for metrics in performance_metrics]\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot recall_train on the left y-axis\n",
    "    ax1.plot(recall_train, 'b-', label='Train Recall')\n",
    "    ax1.set_xlabel('iterations (per hundreds)')\n",
    "    ax1.set_ylabel('Train Recall [%]', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "    # Set x-axis to logarithmic scale\n",
    "    ax1.set_xscale('log')\n",
    "\n",
    "    # Create a second y-axis for recall_test\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(recall_test, 'r-', label='Test Recall')\n",
    "    ax2.set_ylabel('Test Recall [%]', color='r')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Add a title and show the plot\n",
    "    plt.title(f'Recall\\nLearning rate = {lr}')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_performance(performance_metrics, lr = 0.01):\n",
    "    plot_cost_function(performance_metrics, lr)\n",
    "    plot_accuracy(performance_metrics, lr)\n",
    "    plot_recall(performance_metrics, lr)\n",
    "\n",
    "def show_saliency_map(W, b):\n",
    "    X = np.ones((64 * 64 * 3, 1))\n",
    "\n",
    "    Z = W * X + b\n",
    "    A = sigmoid(Z) \n",
    "    A = (255.0 * (A - np.min(A)) / (np.max(A) - np.min(A))).astype(np.uint8)\n",
    "    img = A.reshape(64, 64, 3)\n",
    "\n",
    "    colors = ['Reds', 'Greens', 'Blues']\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes[0, 0].imshow(img)\n",
    "    axes[0, 0].axis('off')  # Hide the axis\n",
    "    axes[0, 0].set_title('Weight-Based Saliency Color Map for Cat vs. Car Classification')\n",
    "    for i in range(3):\n",
    "        axes[1, i].imshow(img[:, :, i], cmap=colors[i], norm=Normalize(vmin=0, vmax=255))\n",
    "        axes[1, i].axis('off')  # Hide the axis\n",
    "        axes[1, i].set_title(f'{colors[i]}')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, dataset, lr=0.00001, iter=10000, labels=['Car', 'Dog'], params_init='min', invert_labels=False):\n",
    "        self.Xtrain, self.Xtest, self.Ytrain, self.Ytest = dataset\n",
    "        self.nx, self.m = self.Xtrain.shape\n",
    "        if invert_labels == True:\n",
    "            self.Ytrain = 1. - self.Ytrain\n",
    "            self.Ytest = 1. - self.Ytest\n",
    "        self.lr = lr\n",
    "        self.iter = iter\n",
    "        self.labels = labels\n",
    "        self.params_init = params_init\n",
    "        self.invert_labels = invert_labels\n",
    "        self.W, self.b = self.init_parameters(self.nx, self.params_init) # W = (n, 1)\n",
    "        self.train()\n",
    "\n",
    "    def init_parameters(self, nx, min_or_max):\n",
    "        if min_or_max == 'min':\n",
    "            W = np.zeros((nx, 1), dtype='float32')\n",
    "        else:\n",
    "            W = np.ones((nx, 1), dtype='float32')\n",
    "        b = 0.0\n",
    "\n",
    "        return W, b\n",
    "\n",
    "    def train(self):\n",
    "        t = time.time()\n",
    "        performance_metrics = []\n",
    "        for i in range(self.iter):\n",
    "            # Forward propagation\n",
    "            Z = linear_combination(self.Xtrain, self.W, self.b) # Z = (1, m)\n",
    "            A = sigmoid(Z) # A = (1, m)\n",
    "\n",
    "            # Compute cost function and model's accuracy\n",
    "            if i % 100 == 0:\n",
    "                J = compute_cost(self.Ytrain, A)\n",
    "                acc_train, acc_test = compute_accuracy(self.Xtrain, self.Xtest, self.Ytrain, self.Ytest, self.W, self.b)\n",
    "                recall_train, recall_test = compute_recall(self.Xtrain, self.Xtest, self.Ytrain, self.Ytest, self.W, self.b)\n",
    "                performance_metrics.append((J, acc_train, acc_test, recall_train, recall_test))\n",
    "                print(f'Iteration {i} out of {self.iter}')\n",
    "\n",
    "            # Backward propagation\n",
    "            dW = np.dot(self.Xtrain, (A - self.Ytrain).T) / self.m # dW = (n, 1)\n",
    "            db = np.sum(A - self.Ytrain) / self.m\n",
    "            self.W -= self.lr * dW\n",
    "            self.b -= self.lr * db\n",
    "            \n",
    "            print(f'time: {((time.time() - t)*1000.):.1f}ms')\n",
    "            t = time.time()\n",
    "\n",
    "        plot_performance(performance_metrics, self.lr)\n",
    "        # show_saliency_map(W, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(show_samples=True)\n",
    "\n",
    "model4 = LogisticRegression(dataset, lr=0.00001, iter=10000, params_init='max', invert_labels=True)\n",
    "\n",
    "model1 = LogisticRegression(dataset, lr=0.00001, iter=10000, params_init='min', invert_labels=False)\n",
    "\n",
    "model2 = LogisticRegression(dataset, lr=0.00001, iter=10000, params_init='min', invert_labels=True)\n",
    "\n",
    "model3 = LogisticRegression(dataset, lr=0.00001, iter=10000, params_init='max', invert_labels=False)\n",
    "\n",
    "\n",
    "\n",
    "'''if INVERT_LABELS == True:\n",
    "    print('Labels: 0 for dogs, 1 for cars')\n",
    "else:\n",
    "    print('Labels: 1 for dogs, 0 for cars')\n",
    "if PARAMS_INIT == 'min':\n",
    "    print('Set parameters to zero')\n",
    "else:\n",
    "    print('Set parameters to one')\n",
    "print(f'  Accuracy: train = {acc_train:.1f}%, test = {acc_test:.1f}%')\n",
    "print(f'  Recall: train = {recall_train:.1f}%, test = {recall_test:.1f}%\\n')'''\n",
    "\n",
    "# print(W)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* True Positive (TP): The model correctly predicted that there was a dog in the image when there actually was a dog.\n",
    "* True Negative (TN): The model correctly predicted that there was no dog in the image when there actually was no dog.\n",
    "* False Positive (FP): The model incorrectly predicted a dog in the image when there actually was no dog (this is often called a \"Type I error\").\n",
    "* False Negative (FN): The model incorrectly predicted no dog in the image when there actually was a dog (this is often called a \"Type II error\").\n",
    "* Accuracy = (True Positives + True Negatives) / Total predictions\n",
    "* Recall = True Positives / (True Positives + False Negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3+meta"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
